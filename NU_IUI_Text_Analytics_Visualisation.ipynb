{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPU9BxsHIFaWBC7Dm6OcjAg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eaby/NLP_Codes/blob/main/NU_IUI_Text_Analytics_Visualisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Text Analytics & Visualisation***"
      ],
      "metadata": {
        "id": "EQPv1BdKsaKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "********************************************************************************\n",
        "\n",
        "# **Task 1 : Word Cloud**\n",
        "\n",
        "Text visualization in Natural Language Processing (NLP) refers to the use of graphical or visual representations to depict and convey information extracted from textual data. It serves as a powerful tool for making sense of large volumes of text, revealing patterns, relationships, and insights that might be challenging to discern through traditional text-based analysis alone. Text visualization techniques help researchers, analysts, and decision-makers gain a more intuitive and comprehensive understanding of textual data.\n",
        "\n",
        "We are going to use WordCloud library to generate a word cloud visualization from a given text in the code.\n",
        "\n",
        "Libraries Used:\n",
        "**'numpy'** is imported as **'np'**. Although not used in this specific code, it's a common practice to import **'numpy'** as **'np'** for mathematical operations.\n",
        "**matplotlib.pyplot** is imported as **'plt'** to create and display the word cloud visualization.\n",
        "**WordCloud** and **STOPWORDS** are imported from the **'wordcloud'** library. **WordCloud** is the primary class for generating word clouds, and **STOPWORDS** contains a set of common English words that are often excluded from word clouds because they don't convey significant meaning.\n",
        "\n",
        "\n",
        "**generate_wordcloud** is a function that takes one argument, **'text'**, which is the input text from which the word cloud will be generated. It first creates a set of stopwords using the **STOPWORDS** set from the **wordcloud** library. These stopwords are words like \"the,\" \"is,\" \"and,\" etc., which are typically filtered out of word clouds. Then, it initializes a **WordCloud** object with the following parameters:\n",
        "\n",
        "\n",
        "> **background_color='white'**: Sets the background color of the word cloud to white.\n",
        "\n",
        ">**stopwords=stopwords**: Specifies the set of stopwords to be used.\n",
        "\n",
        ">**max_words=200**: Limits the maximum number of words to be displayed in the word cloud to 200.\n",
        "\n",
        ">**max_font_size=40**: Sets the maximum font size for words in the word cloud to 40.\n",
        "\n",
        ">**random_state=42**: Provides seed for the random number generator, ensuring reproducibility of the word cloud layout.\n",
        "\n",
        "\n",
        "The WordCloud object is then used to generate the word cloud from the input text and is displayed using **matplotlib.pyplot** with a specified figure size, and the imshow function is used to show the word cloud. **plt.axis('off')** removes the axis labels and ticks from the plot, and **plt.show()** displays the plot\n",
        "\n",
        "sample_text is the variable containing the sample text from which the word cloud will be generated. It is a multi-line string containing a description. You can replace this static text input with real-time text sources or other text files with large content.\n",
        "\n",
        "When we run this code, it will generate a word cloud based on the words in the sample_text and display it as a graphical representation where the most frequent words will be larger and more prominent in the visualization."
      ],
      "metadata": {
        "id": "pyIuLchbsjck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wordcloud matplotlib"
      ],
      "metadata": {
        "id": "emOntbyLtERz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGEVXpF9sJqi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "def generate_wordcloud(text):\n",
        "    stopwords = set(STOPWORDS)\n",
        "    wordcloud = WordCloud(\n",
        "        background_color='white',\n",
        "        stopwords=stopwords,\n",
        "        max_words=200,\n",
        "        max_font_size=40,\n",
        "        random_state=42\n",
        "    ).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "sample_text = \"\"\"\n",
        "    Natural language processing is a sub-field of artificial intelligence,\n",
        "    in which its depth involves the interactions between computers and humans.\n",
        "    Through NLP, it is possible for computers to read text, interpret it,\n",
        "    measure sentiment and determine which parts are important.\n",
        "\"\"\"\n",
        "\n",
        "generate_wordcloud(sample_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** You can customize the appearance and behavior of the word cloud using various parameters in the WordCloud class. For instance, background_color sets the background color, max_words sets the maximum number of words to display, and max_font_size sets the largest font size in the word cloud.\n",
        "\n",
        "Note: If you want to use a custom shape or mask for your word cloud, you can utilize the mask parameter of WordCloud and pass a binary image (with the shape you want as white and the rest as black). This will restrict the word cloud to the shape of the white region in the image.\n"
      ],
      "metadata": {
        "id": "vhvb3hZUthq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "********************************************************************************\n",
        "\n",
        "# **Task 2 : Heatmaps**\n",
        "\n",
        "Heatmaps are a common data visualization technique used in Natural Language Processing (NLP) to represent relationships, patterns, or distributions within textual data. In a heatmap, data values are displayed as a grid of colored squares, with each square's color intensity representing the magnitude of a specific value or the degree of association between two variables. In the context of NLP, heatmaps can be applied in various ways including Word Co-occurrence, Text Similarity, Word Frequency, and Sequence Analysis.\n",
        "\n",
        "The Python script below performs these 4 activities. The description for the code is explained below.\n",
        "\n",
        ">**Sample Text Data:**\n",
        "**text_data**: A list of sample text sentences for analysis. These sentences represent different aspects of natural language processing and machine learning.\n",
        "\n",
        ">**Tokenization of Text Data:**\n",
        "**tokenized_data:** Tokenization is the process of splitting text into words or tokens. In this code, each sentence in **text_data** is tokenized into a list of words for further analysis.\n",
        "\n",
        ">**Function to Generate and Display Heatmaps:**\n",
        "**generate_heatmap(data, labels, title)**: A function that generates and displays a customized heatmap using Seaborn and Matplotlib. It takes data, labels, and a title as input parameters and displays a heatmap with annotations.\n",
        "\n",
        ">**Word Frequency Heatmap:**\n",
        "Calculates the word frequency of all words in the tokenized_data.\n",
        "Selects the top N most frequent words (defined by top_n_words).\n",
        "Generates and displays a Word Frequency Heatmap using the generate_heatmap function.\n",
        "\n",
        ">**Word Co-occurrence Heatmap:**\n",
        "Calculates the co-occurrence matrix of words in the text_data.\n",
        "Selects the top N most relevant words for co-occurrence (defined by top_n_words_cooc).\n",
        "Generates and displays a Word Co-occurrence Heatmap using the generate_heatmap function.\n",
        "\n",
        ">**Text Similarity Heatmap:**\n",
        "Uses TF-IDF (Term Frequency-Inverse Document Frequency) to calculate the similarity between text documents.\n",
        "Generates and displays a Text Similarity Heatmap using the generate_heatmap function.\n",
        "\n",
        ">**Print Document Indices and Corresponding Text Data:**\n",
        "Prints the document indices and their corresponding text data to help users understand the numbering used in the Text Similarity Heatmap.\n",
        "\n",
        ">**Sequence Analysis Heatmap (Word Presence):**\n",
        "Creates a binary matrix to represent the presence or absence of words in each sentence.\n",
        "Generates and displays a Sequence Analysis Heatmap (Word Presence) using the generate_heatmap function.\n",
        "\n",
        ">**Print Legend for Sequence Analysis Heatmap (Word Presence):**\n",
        "Prints a legend to explain the meaning of 0 and 1 in the Sequence Analysis Heatmap.\n",
        "Additionally, it provides information about the axes, including the X and Y axes."
      ],
      "metadata": {
        "id": "Ag0NYtABygq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk import FreqDist\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Different sample text data\n",
        "text_data = [\n",
        "    \"Social media platforms connect people from all over the world.\",\n",
        "    \"Machine learning algorithms can analyze large datasets efficiently.\",\n",
        "    \"Text summarization techniques condense lengthy documents into concise summaries.\",\n",
        "    \"Deep learning models have revolutionized image and speech recognition.\",\n",
        "    \"Natural language generation can automatically create human-like text.\",\n",
        "]\n",
        "\n",
        "# Tokenize the text data (for sequence analysis)\n",
        "tokenized_data = [sentence.split() for sentence in text_data]\n",
        "\n",
        "# Function to generate and display a customized heatmap\n",
        "def generate_heatmap(data, labels, title):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.set(font_scale=1.2)\n",
        "    sns.set_style(\"whitegrid\")  # Customize the plot style\n",
        "    sns.heatmap(data, annot=True, fmt=\".2f\", cmap=\"Blues\", linewidths=1, linecolor='black', cbar=False,\n",
        "                xticklabels=labels, yticklabels=labels, annot_kws={\"size\": 12})\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Word Frequency Heatmap with top N most frequent words\n",
        "top_n_words = 5  # Set the number of top words to display\n",
        "word_freq = FreqDist(np.concatenate(tokenized_data))\n",
        "most_common_words = word_freq.most_common(top_n_words)  # Get the most frequent words\n",
        "word_labels = [word[0] for word in most_common_words]  # Extract word labels\n",
        "word_freq_matrix = np.array([[word_freq[word] for word in word_labels]])\n",
        "generate_heatmap(word_freq_matrix, word_labels, \"Word Frequency map (Top {} Words)\".format(top_n_words))\n",
        "\n",
        "# Word Co-occurrence Heatmap with top N most relevant words\n",
        "top_n_words_cooc = 10  # Set the number of top words for co-occurrence\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "co_occurrence_matrix = vectorizer.fit_transform(text_data).T.dot(vectorizer.fit_transform(text_data))\n",
        "word_cooc_labels = vectorizer.get_feature_names_out()\n",
        "word_cooc_matrix = co_occurrence_matrix[:top_n_words_cooc, :top_n_words_cooc]\n",
        "generate_heatmap(word_cooc_matrix.toarray(), word_cooc_labels[:top_n_words_cooc], \"Word Co-occurrence map (Top {} Words)\".format(top_n_words_cooc))\n",
        "\n",
        "# Text Similarity Heatmap\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)\n",
        "text_similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "generate_heatmap(text_similarity_matrix, range(len(text_data)), \"Text Similarity map\")\n",
        "\n",
        "# Print document indices and corresponding text data\n",
        "print(\"Document Indices and Corresponding Text Data:\")\n",
        "for i, sentence in enumerate(text_data):\n",
        "    print(f\"{i}: {sentence}\")\n",
        "\n",
        "# Sequence Analysis Heatmap (using word presence)\n",
        "sequence_matrix = np.zeros((len(tokenized_data), len(tokenized_data[0])))\n",
        "for i, sentence in enumerate(tokenized_data):\n",
        "    for j, word in enumerate(sentence):\n",
        "        if word in text_data[i]:\n",
        "            sequence_matrix[i][j] = 1\n",
        "\n",
        "# Print legend for Sequence Analysis Heatmap (Word Presence)\n",
        "print(\"\\nLegend for Sequence Analysis Heatmap (Word Presence):\")\n",
        "print(\"0: Word is not present in the sentence.\")\n",
        "print(\"1: Word is present in the sentence.\")\n",
        "print(\"-----------------------------------\")\n",
        "print(\"X axis: Total number of words in each text\")\n",
        "print(\"Y axis: Total number of text\")\n",
        "\n",
        "generate_heatmap(sequence_matrix, range(len(tokenized_data[0])), \"Sequence Analysis Heatmap (Word Presence)\")"
      ],
      "metadata": {
        "id": "v7ERl0ul0n-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "********************************************************************************\n",
        "\n",
        "# **Task 3 : Geospatial Visualization**  \n",
        "\n",
        "Geospatial Visualization in the context of Natural Language Processing (NLP) refers to the visualization of location-based or geographic data that is derived from textual sources. This can help to present insights and patterns related to locations, movements, or any spatial context mentioned in the text.\n",
        "\n",
        "Geospatial Visualization with NLP can be used to do the below tasks:\n",
        "\n",
        "**Information Extraction:** NLP techniques are used to extract location-based information from textual sources. This could be place names, addresses, coordinates, or even contextual information related to places. Named Entity Recognition (NER) is a common NLP task for extracting such named entities, including locations.\n",
        "\n",
        "**Geocoding:** After extracting place names, these need to be converted to geographic coordinates (latitude and longitude) for visualization. This process is called geocoding and often involves third-party services or datasets. Conversely, reverse geocoding converts coordinates back to human-readable place names.\n",
        "\n",
        "**Spatial Analysis:** Once you have location-based data, you can perform spatial analysis. This can involve understanding proximity, calculating distances between points, determining clusters, or understanding movement patterns.\n",
        "\n",
        "**Visualization:** The location-based data can be visualized on maps using markers, heatmaps, lines, or polygons. This helps in deriving insights related to spatial distribution, trends, or patterns.\n",
        "\n",
        "Application are **Travel Blogs or Diaries** (Extract and visualize places mentioned in a travelogue to trace the journey on a map)\n",
        "**News Analysis** (Identify and visualize the geographic distribution of news events or stories)\n",
        "**Social Media Analysis** (Understand where users are talking about specific topics by analyzing geotagged tweets or posts)\n",
        "**Literature or Historical Analysis** (Map places mentioned in historical documents or novels to get a spatial understanding of events or narratives)\n",
        "\n",
        "Tools & Libraries that can be used are **geopy** for geocoding and **folium** for map-based visualization are popular used with **python** and tools like **QGIS** or **ArcGIS** can be used for more advanced geospatial analysis and visualization which are **GIS Software**.\n",
        "\n",
        "In general, Geospatial Visualization in NLP involves extracting location information from textual data and presenting it visually on maps or other spatial formats, enabling richer analysis and insights related to geographic or spatial context.\n",
        "\n",
        "We have 2 code segments below 3(a) and 3(b) which are used to understand the concept of Geospatial Visualization.\n"
      ],
      "metadata": {
        "id": "PeLgMtXntjm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<><><><><><><><><><><><><><><><><><><>>\n",
        "# **Task 3(a) : Geospatial Visualization without NLP**  \n",
        "\n",
        "Lets deal with this task as a solution for problem statemnt\n",
        "\n",
        "**Problem Statement: Geospatial Visualization of Personal Travel Diary**\n",
        "\n",
        "**Background:**\n",
        "In the age of digital content and blogging, many individuals document their travel experiences in digital formats such as blogs, online journals, or social media posts. These textual narrations often include dates and names of places visited. While these narratives are rich in detail, they lack an interactive and visual perspective that would allow readers or the authors themselves to visually trace their journey over time.\n",
        "\n",
        "**Objective:**\n",
        "Develop a tool that extracts mentions of dates and places from a given textual travel diary and then visualizes these visits on an interactive map. The visual representation should allow users to see the sequence of visits as they happened over time.\n",
        "\n",
        "**Requirements:**\n",
        "The tool should accept a multiline text input describing travel experiences with mentions of dates and places.\n",
        "The tool should be able to recognize and extract date and place mentions from the given input.\n",
        "For each recognized place, the tool should determine its geographical coordinates.\n",
        "The tool should visualize the extracted places on an interactive map.\n",
        "The visual representation on the map should be timestamped, allowing for an animated playback of the travel journey.\n",
        "Each place on the map should have a distinct visual representation (e.g., different colors) to distinguish between the various visits.\n",
        "The resultant map should be saved as an interactive HTML file for easy sharing and viewing.\n",
        "\n",
        "**Constraints:**\n",
        "The initial version of the tool should focus on places located in the UK.\n",
        "The tool should respect rate limits when fetching geographical coordinates to avoid overloading geocoding services.\n",
        "The tool should handle scenarios where a place's geographical coordinates cannot be determined.\n",
        "\n",
        "**Desired Outcome:**\n",
        "At the end of this project, a user should be able to provide a text-based travel diary and receive an interactive geospatial visualization in HTML format, illustrating their journey across the UK over time.\n",
        "\n",
        "The code below provided is the solution for the above stated application, it creates an interactive geospatial visualization to depict the sequence of places visited by a person in the UK, as mentioned in the input_text. The resulting map animates through the sequence of visits with different colored points for each location.\n",
        "\n",
        "\n",
        "Code description is provided below.\n",
        "\n",
        "**Libraries Used:**\n",
        "\n",
        "**folium:** A Python library for creating interactive maps.\n",
        "\n",
        "**TimestampedGeoJson:** A plugin for folium to visualize data with timestamps.\n",
        "\n",
        "**Nominatim:** A geocoding service from the geopy library that converts place names to latitude and longitude.\n",
        "\n",
        "**datetime and time:** Python built-in libraries for date and time operations.\n",
        "\n",
        "**re:** Python built-in library for regular expression operations.\n",
        "\n",
        "**input_text:** A multi-line string containing dates followed by place names.\n",
        "\n",
        "The **re.findall()** function uses a regular expression to extract dates, verbs (e.g., \"visited\", \"went\"), and place names from **input_text**.\n",
        "\n",
        "For converting Place Names to Latitude and Longitude, the code initializes the **Nominatim** geolocator. For each extracted place name, the geolocator gets the latitude and longitude. A **time.sleep(1)** is used to prevent hitting the geolocator's rate limits.\n",
        "\n",
        "A base map centered around the UK is created using **folium.Map()**.\n",
        "\n",
        "For timestamped data Preparation, the code prepares data to be visualized as points with timestamps. Each location and its associated date are matched with a color from the predefined list of colors.\n",
        "The **data** dictionary, formatted as GeoJSON, contains each place as a feature with a point geometry (latitude and longitude) and properties (timestamp and visualization style).\n",
        "\n",
        "For Adding Timestamped Data to the Map, we use the **TimestampedGeoJson** plugin is used to visualize the data points with timestamps on the base map. The period is set to \"P1D\" which means 1 day, so the data points will be visualized one day at a time. Various other parameters control the appearance and behaviour of the timeline slider.\n",
        "\n",
        "Finally, the interactive map with the visualized timeline is saved to an HTML file named **\"Visited_Places_Geospatial_Visualization_v2.html\"**.\n"
      ],
      "metadata": {
        "id": "8Ykj6Ya3wY_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3(a) Geospatial Visualization without NLP\n",
        "\n",
        "import folium\n",
        "from folium.plugins import TimestampedGeoJson\n",
        "from geopy.geocoders import Nominatim\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "\n",
        "# Sample text input with 10 places in the UK\n",
        "input_text = \"\"\"\n",
        "On 2023-01-01, I visited London. On 2023-01-02, I went to Edinburgh.\n",
        "On 2023-01-03, I traveled to Cardiff. On 2023-01-04, I enjoyed Belfast.\n",
        "On 2023-01-05, I explored Liverpool. On 2023-01-06, I saw Manchester.\n",
        "On 2023-01-07, I was in Birmingham. On 2023-01-08, I roamed around Bristol.\n",
        "On 2023-01-09, I stopped by Cambridge. On 2023-01-10, I checked out Oxford.\n",
        "\"\"\"\n",
        "\n",
        "# Extract places and dates from the text input\n",
        "places_dates = re.findall(r\"On (\\d{4}-\\d{2}-\\d{2}), I.*? (visited|went|traveled|enjoyed|explored|saw|was|roamed|stopped by|checked out) (.*?)\\.\", input_text)\n",
        "\n",
        "# Convert place names to latitude and longitude\n",
        "geolocator = Nominatim(user_agent=\"timelineGeocoder_v4\")\n",
        "locations = []\n",
        "\n",
        "for date_str, verb, place in places_dates:\n",
        "    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "    location = geolocator.geocode(place + \", United Kingdom\")\n",
        "    if location:\n",
        "        locations.append((date, (location.latitude, location.longitude)))\n",
        "    time.sleep(1)\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[54, -3], zoom_start=6)\n",
        "\n",
        "# List of colors\n",
        "colors = [\"red\", \"blue\", \"green\", \"purple\", \"orange\", \"darkred\", \"darkblue\", \"darkgreen\", \"cadetblue\", \"darkpurple\", \"pink\"]\n",
        "\n",
        "# Create data for TimestampedGeoJson\n",
        "data = {\n",
        "    \"type\": \"FeatureCollection\",\n",
        "    \"features\": []\n",
        "}\n",
        "\n",
        "for (date, coords), color in zip(locations, colors):\n",
        "    feature = {\n",
        "        \"type\": \"Feature\",\n",
        "        \"geometry\": {\n",
        "            \"type\": \"Point\",\n",
        "            \"coordinates\": [coords[1], coords[0]]\n",
        "        },\n",
        "        \"properties\": {\n",
        "            \"times\": [date.strftime(\"%Y-%m-%dT%H:%M:%S\")],\n",
        "            \"icon\": \"circle\",\n",
        "            \"iconstyle\": {\n",
        "                \"fillColor\": color,\n",
        "                \"fillOpacity\": 0.8,\n",
        "                \"stroke\": \"true\",\n",
        "                \"radius\": 7\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    data[\"features\"].append(feature)\n",
        "\n",
        "# Add TimestampedGeoJson to the map\n",
        "TimestampedGeoJson(\n",
        "    data,\n",
        "    period=\"P1D\",\n",
        "    add_last_point=True,\n",
        "    auto_play=True,\n",
        "    loop=False,\n",
        "    max_speed=1,\n",
        "    loop_button=True,\n",
        "    date_options='YYYY-MM-DD',\n",
        "    time_slider_drag_update=True\n",
        ").add_to(m)\n",
        "\n",
        "m.save(\"Visited_Places_Geospatial_Visualization_v2.html\")\n"
      ],
      "metadata": {
        "id": "KX_uYfaEjPKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<><><><><><><><><><><><><><><><><><><>>\n",
        "# **Task 3(b) : Geospatial Visualization NLP**  \n",
        "\n",
        "**Problem Statement:**\n",
        "Develop an application that takes in textual descriptions of a traveler's journey, with dates, places visited, and sentiments about each location.\n",
        "\n",
        "**The application should:**\n",
        "Extract the essential data, i.e., dates, locations, and sentiments from the textual descriptions.\n",
        "Analyze the sentiments associated with each visited location using natural language processing techniques.\n",
        "Geocode the locations to get their corresponding geographical coordinates.\n",
        "Visually represent the journey on a map, indicating the sequence of visits with directional lines and using color-coded markers based on the analyzed sentiments.\n",
        "\n",
        "\n",
        "**Objectives:**\n",
        "Data Extraction: Extract dates, locations, and sentiments from raw text using regular expressions.\n",
        "\n",
        "**Sentiment Analysis:** Employ natural language processing tools to categorize sentiments as positive, negative, or neutral based on textual descriptions.\n",
        "\n",
        "**Geocoding:** Convert textual location descriptors into geographical coordinates.\n",
        "\n",
        "**Visualization:** Create an interactive map:\n",
        "Place markers on the map for each location.\n",
        "Color-code markers based on sentiment: green for positive, yellow for neutral, and red for negative.\n",
        "Draw interconnecting lines between markers to represent the journey's sequence, with arrows to indicate direction.\n",
        "\n",
        "**Deliverables:**\n",
        "An interactive HTML map showcasing the journey and associated sentiments.\n",
        "A user-friendly interface for inputting travel journal descriptions.\n",
        "\n",
        "**Challenges:**\n",
        "Handling variations in textual descriptions, e.g., different phrases used to denote visiting a place.\n",
        "Ensuring accurate sentiment analysis, given the subjective nature of sentiments.\n",
        "Addressing potential geocoding inaccuracies or failures.\n",
        "Ensuring that the visualization remains clear and interpretable even if there are many locations or overlapping routes.\n",
        "\n",
        "**Benefits:**\n",
        "By implementing this application, users can get a comprehensive visual overview of their journey across different places, enriched by their feelings and experiences. It can serve as an enhanced digital travel diary, aiding in reminiscing or sharing travel experiences with others.\n",
        "\n",
        "**Code Description:**\n",
        "\n",
        "**folium:** This library is used for rendering leaflet maps.\n",
        "\n",
        "**datetime and time:** To work with date and time.\n",
        "\n",
        "**re:** For regular expression operations.\n",
        "\n",
        "**nltk.sentiment:** Contains the sentiment analysis tool **'SentimentIntensityAnalyzer'**.\n",
        "\n",
        "**nltk:** Library for natural language processing.\n",
        "\n",
        "**geopy.geocoders:** To geocode locations into latitude and longitude.\n",
        "\n",
        "**folium.plugins:** Plugins to beautify and enhance folium visualizations.\n",
        "\n",
        "For Sentiment Analysis, we use NLTK's **'SentimentIntensityAnalyzer'**, it analyzes the sentiment of the description for each place visited and based on the compound score, the sentiment is classified as positive, negative, or neutral.\n",
        "\n",
        "The we do Data Extraction, a regular expression pattern is used to extract dates, places, and descriptions from the **'input_text'**. For each extracted place, the **'Nominatim'** geocoder is used to get the latitude and longitude. These are used later for plotting on the map.\n",
        "\n",
        "For Map creation, a base folium map centred around the UK is created.\n",
        "For each location (sorted by date), a marker is placed on the map. The number on the marker indicates the order of visits, and its color reflects the sentiment (green for positive, yellow for neutral, and red for negative).\n",
        "Lines are drawn to interconnect the places in the order they were visited. These lines have arrows using **'PolyLineTextPath@** to show the direction of the journey.\n",
        "\n",
        "Last the final map visualization is saved as an HTML file named **\"Enhanced_Sentiment_Geospatial_Visualizations_v3.html\"**.\n"
      ],
      "metadata": {
        "id": "mMnGQQ_2w5li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3(b) : Geospatial Visualization NLP\n",
        "\n",
        "import folium\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "from folium.plugins import PolyLineTextPath, BeautifyIcon\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "# Sample text input with sentiments for each place\n",
        "input_text = \"\"\"\n",
        "On 2023-01-01, I visited London, United Kingdom, and it was absolutely breathtaking.\n",
        "On 2023-01-02, I went to Edinburgh, Scotland, and I found it very boring.\n",
        "On 2023-01-03, I traveled to Cardiff, Wales, which was quite delightful.\n",
        "On 2023-01-04, I enjoyed Belfast, Northern Ireland, although it was a bit underwhelming.\n",
        "On 2023-01-05, I explored Liverpool, England, and it was an amazing experience.\n",
        "On 2023-01-06, I saw Manchester, England, which was okay.\n",
        "On 2023-01-07, I was in Birmingham, England, and didn't like it much.\n",
        "On 2023-01-08, I roamed around Bristol, England, which was exciting.\n",
        "On 2023-01-09, I stopped by Cambridge, England, which was decent.\n",
        "On 2023-01-10, I checked out Oxford, England, and it was fabulous.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize sentiment analysis tool\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Extract sentiments, places, and dates from the text input\n",
        "pattern = r\"On (\\d{4}-\\d{2}-\\d{2}), I.*?(?:visited|went to|traveled to|enjoyed|explored|saw|was in|roamed around|stopped by|checked out) (.*?)(?: and |, |\\.)(.*?)(?:\\.|$)\"\n",
        "places_dates_descriptions = re.findall(pattern, input_text)\n",
        "\n",
        "# Set up the Nominatim geocoder\n",
        "geolocator = Nominatim(user_agent=\"timelineGeocoder_v10\", timeout=10)\n",
        "\n",
        "locations = []\n",
        "for date_str, place, description in places_dates_descriptions:\n",
        "    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "\n",
        "    sentiment = \"neutral\"\n",
        "    sentiment_score = sia.polarity_scores(description)[\"compound\"]\n",
        "    if sentiment_score > 0.05:\n",
        "        sentiment = \"positive\"\n",
        "    elif sentiment_score < -0.05:\n",
        "        sentiment = \"negative\"\n",
        "\n",
        "    try:\n",
        "        location = geolocator.geocode(place + \", United Kingdom\")\n",
        "        if location:\n",
        "            locations.append((date, place.strip(), (location.latitude, location.longitude), sentiment))\n",
        "    except Exception as e:\n",
        "        print(f\"Error geocoding {place}: {str(e)}\")\n",
        "\n",
        "    time.sleep(1)  # Adjust the waiting time if needed to be polite to the service\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[54, -3], zoom_start=6, tiles=\"CartoDB Positron\")\n",
        "\n",
        "# Color mapping based on sentiment\n",
        "color_map = {\n",
        "    \"positive\": \"#00FF00\",  # Green\n",
        "    \"neutral\": \"#FFFF00\",  # Yellow\n",
        "    \"negative\": \"#FF0000\"  # Red\n",
        "}\n",
        "\n",
        "coords_list = []\n",
        "for idx, (date, place, coords, sentiment) in enumerate(sorted(locations, key=lambda x: x[0])):\n",
        "    folium.Marker(\n",
        "        location=coords,\n",
        "        icon=BeautifyIcon(\n",
        "            icon_shape='marker',\n",
        "            border_color=color_map[sentiment],\n",
        "            border_width=1,\n",
        "            text_color=color_map[sentiment],\n",
        "            number=idx+1,\n",
        "            inner_icon_style='margin-top:0px;'\n",
        "        ),\n",
        "        popup=f\"{date.strftime('%Y-%m-%d')} - {place} ({sentiment})\"\n",
        "    ).add_to(m)\n",
        "\n",
        "    coords_list.append(coords)\n",
        "\n",
        "# Draw interconnecting lines\n",
        "polyline = folium.PolyLine(coords_list, color=\"#00ABDC\", weight=4, opacity=0.7).add_to(m)\n",
        "PolyLineTextPath(polyline, 'â†’', offset=10, repeat=True, font_size=18, font_weight='bold').add_to(m)\n",
        "\n",
        "m.save(\"Enhanced_Sentiment_Geospatial_Visualizations_v3.html\")"
      ],
      "metadata": {
        "id": "wrCKA5xn4m2O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}