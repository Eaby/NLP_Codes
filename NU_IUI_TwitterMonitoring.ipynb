{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM88P3c4oQcHgNlfxDpdO7W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eaby/NLP_Codes/blob/main/NU_IUI_TwitterMonitoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Twitter (X platform) Monitoring***\n",
        "\n",
        "********************************************************************************\n",
        "\n",
        "# **Task 1 : Trend Detection**\n",
        "\n",
        "Twitter monitoring for trend detection using Natural Language Processing (NLP) involves collecting, analyzing, and interpreting Twitter data to identify emerging topics, sentiments, or patterns. The main steps involved are **Data Collection, Preprocessing, Feature Extraction, Trend Detection, Visualization, Alerts and Notifications**.\n",
        "NLP can be a powerful tool for trend detection, human interpretation and judgment are essential. Always contextualize findings and consider external factors that might influence trends.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "You'll need download the dataset from the below google drive location and save it in you google drive to run the code.\n",
        "\n",
        ">Dataset Link: https://drive.google.com/file/d/13tjdXgX3cSyw-IdJiRvfyx_8p92zs_S2/view?usp=sharing\n",
        "\n",
        ">File name of the dataset: **training.1600000.processed.noemoticon.csv**\n",
        "\n",
        "Once the file is in you google drive,  replace the file_path in the code below with your correct file path of the dataset for executing the code.\n"
      ],
      "metadata": {
        "id": "4Yk3QoybggFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas nltk"
      ],
      "metadata": {
        "id": "u25bRt795iUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#************************************************************************************************\n",
        "# Developer: Eaby Kollonoor Babu\n",
        "# Version: 2.1\n",
        "# Last Updated: 2023-09-12\n",
        "# Contact:eaby.asha@gmail.com\n",
        "\n",
        "# Description\n",
        "\"\"\"\n",
        "A simple Twitter monitoring program using Python(for educational purpose only)\n",
        "\n",
        "Its a program to do trend detection using Natural Language Processing (NLP) involves collecting,\n",
        "analyzing, and interpreting Twitter data to identify emerging topics, sentiments, or patterns. .\n",
        "\"\"\"\n",
        "\n",
        "# License and Copyright Notice\n",
        "\"\"\"\n",
        "Copyright (c) 2023 Eaby Kollonoor Babu\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
        "this software and associated documentation files (the \"Software\"), for the sole\n",
        "purpose of educational and non-commercial use, without restriction, including\n",
        "without limitation the rights to use, copy, modify, merge, publish, distribute,\n",
        "or sublicense copies of the Software, and to permit persons to whom the Software\n",
        "is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\" FOR EDUCATIONAL USE ONLY, WITHOUT WARRANTY OF ANY\n",
        "KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
        "MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO\n",
        "EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n",
        "OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "FROM, OUT OF, OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n",
        "IN THE SOFTWARE.\n",
        "\"\"\"\n",
        "\n",
        "# Changelog/Release Notes\n",
        "\"\"\"\n",
        "Changelog:\n",
        "\n",
        "- Version 1.0 (2023-08-02): Twitter data extraction code.\n",
        "- Version 1.1 (2023-08-11): Added data formating by Tokenization, Lemmatization, and cleaning.\n",
        "- Version 2.1 (2023-09-12): Added Sentiment Analysis to the code.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Feedback\n",
        "\"\"\"\n",
        "For questions or feedback, feel free to email me at eaby.asha@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "#************************************************************************************************\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.util import bigrams\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ensure you've downloaded the necessary NLTK data\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Set the path to your dataset on Google Drive\n",
        "file_path = '/content/drive/My Drive/MyData/training.1600000.processed.noemoticon.csv'\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(file_path, encoding='ISO-8859-1', header=None)\n",
        "data.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
        "\n",
        "# Take input keyword from user\n",
        "keyword = input(\"Enter the keyword to analyze trends for: \")\n",
        "top_n = int(input(\"Enter the number of top trends you want to view (e.g. 10): \"))\n",
        "\n",
        "# Filter the dataset for tweets containing the input keyword\n",
        "data = data[data['text'].str.contains(keyword, case=False, na=False)]\n",
        "\n",
        "# If no tweets found for the given keyword, exit\n",
        "if data.empty:\n",
        "    print(f\"No tweets found for keyword: {keyword}\")\n",
        "    exit()\n",
        "\n",
        "# Tokenization, Lemmatization, and cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "all_words = []\n",
        "for text in data['text']:\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha() and word not in stop_words]\n",
        "    all_words.extend(words)\n",
        "\n",
        "# Incorporate bigrams\n",
        "all_bigrams = list(bigrams(all_words))\n",
        "bigram_freq = Counter(all_bigrams)\n",
        "top_bigrams = bigram_freq.most_common(top_n)\n",
        "\n",
        "# Get the most common words associated with the keyword\n",
        "word_freq = Counter(all_words)\n",
        "trends = word_freq.most_common(top_n)  # Top N trends\n",
        "\n",
        "# If no tweets found for the given keyword, exit\n",
        "if data.empty:\n",
        "    print(f\"No tweets found for keyword: {keyword}\")\n",
        "    exit()\n",
        "\n",
        "# ... your tokenization and cleaning code ...\n",
        "\n",
        "# Check if trends list is empty\n",
        "if not trends:\n",
        "    print(f\"No significant words found for the keyword: {keyword} in the sampled dataset.\")\n",
        "else:\n",
        "    # Visualization\n",
        "    words, counts = zip(*trends)\n",
        "    plt.figure(figsize=(15,7))\n",
        "    plt.bar(words, counts, color='skyblue')\n",
        "    plt.title(f\"Top {top_n} Trends associated with '{keyword}'\")\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Display top bigrams\n",
        "print(f\"\\nTop {top_n} Bigrams associated with '{keyword}':\")\n",
        "for bigram, freq in top_bigrams:\n",
        "    print(f\"{bigram}: {freq}\")\n",
        "\n",
        "# Sentiment Analysis\n",
        "positive_tweets = len(data[data['sentiment'] == 4])\n",
        "neutral_tweets = len(data[data['sentiment'] == 2])\n",
        "negative_tweets = len(data[data['sentiment'] == 0])\n",
        "\n",
        "print(f\"\\nSentiment Analysis for '{keyword}':\")\n",
        "print(f\"Positive Tweets: {positive_tweets}\")\n",
        "print(f\"Neutral Tweets: {neutral_tweets}\")\n",
        "print(f\"Negative Tweets: {negative_tweets}\")\n"
      ],
      "metadata": {
        "id": "qMY035255jsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code performs keyword trend analysis and sentiment analysis on a dataset of tweets stored in Google Drive. The main objective of this code is to analyse how frequently certain words and bigrams appear in tweets containing a specific keyword entered by the user and to determine the sentiment of those tweets.\n",
        "\n",
        "Libraries user:\n",
        "**pandas** for data manipulation and analysis.\n",
        "**Counter** to count occurrences of elements.\n",
        "Several functions and modules from the **nltk** library for natural language processing.\n",
        "**matplotlib.pyplot** for data visualization.\n",
        "\n",
        "**NLTK Data Download:**\n",
        "It downloads the necessary data files for the **nltk** library. This includes stopwords, tokenizers, lemmatizers, and POS taggers.\n",
        "\n",
        "**User Input**:\n",
        "The code takes two inputs from the user:\n",
        "A keyword to analyze trends.\n",
        "The number of top trends (words or bigrams) they want to view.\n",
        "\n",
        "**Tokenization, Lemmatization, and Cleaning:**\n",
        "The code performs several text preprocessing steps:\n",
        "Tokenization: splitting text into individual words.\n",
        "Lemmatization: converting words to their base form (e.g., \"running\" to \"run\").\n",
        "Cleaning: removing non-alphanumeric words and stopwords (common words like \"and\", \"the\", etc.).\n",
        "**Bigram Analysis:**\n",
        "After tokenization, the code creates bigrams (pairs of adjacent words) and counts their occurrences.\n",
        "\n",
        "**Top Trends:**\n",
        "The code identifies the top N words (trends) associated with the input keyword.\n",
        "** Visualization:**\n",
        "The top N trends are visualized using a bar chart.\n",
        "\n",
        "**Sentiment Analysis:**\n",
        "The code counts the number of positive, neutral, and negative tweets associated with the keyword and prints out the counts for each sentiment category.\n"
      ],
      "metadata": {
        "id": "imtIOwCC9GzJ"
      }
    }
  ]
}