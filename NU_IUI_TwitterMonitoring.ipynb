{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfRfzXPfuEkO8t3kLss6Pe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eaby/NLP_Codes/blob/main/NU_IUI_TwitterMonitoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Twitter (X platform) Monitoring***"
      ],
      "metadata": {
        "id": "4Yk3QoybggFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "********************************************************************************\n",
        "\n",
        "# **Task 1 : Trend Detection**\n",
        "\n",
        "Twitter monitoring for trend detection using Natural Language Processing (NLP) involves collecting, analyzing, and interpreting Twitter data to identify emerging topics, sentiments, or patterns. The main steps involved are **Data Collection, Preprocessing, Feature Extraction, Trend Detection, Visualization, Alerts and Notifications**.\n",
        "NLP can be a powerful tool for trend detection, human interpretation and judgment are essential. Always contextualize findings and consider external factors that might influence trends.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "You'll need download the dataset from the below google drive location and save it in you google drive to run the code.\n",
        "\n",
        "Dataset Link: https://drive.google.com/file/d/13tjdXgX3cSyw-IdJiRvfyx_8p92zs_S2/view?usp=sharing\n",
        "File name of the dataset: **training.1600000.processed.noemoticon.csv**\n",
        "\n",
        "Once the file is in you google drive,  replace the file_path in the code below with your correct file path of the dataset for executing the code.\n",
        "\n"
      ],
      "metadata": {
        "id": "XQYIZi_WgsJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas nltk"
      ],
      "metadata": {
        "id": "u25bRt795iUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.util import bigrams\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ensure you've downloaded the necessary NLTK data\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Set the path to your dataset on Google Drive\n",
        "file_path = '/content/drive/My Drive/MyData/training.1600000.processed.noemoticon.csv'\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(file_path, encoding='ISO-8859-1', header=None)\n",
        "data.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
        "\n",
        "# Take input keyword from user\n",
        "keyword = input(\"Enter the keyword to analyze trends for: \")\n",
        "top_n = int(input(\"Enter the number of top trends you want to view (e.g. 10): \"))\n",
        "\n",
        "# Filter the dataset for tweets containing the input keyword\n",
        "data = data[data['text'].str.contains(keyword, case=False, na=False)]\n",
        "\n",
        "# If no tweets found for the given keyword, exit\n",
        "if data.empty:\n",
        "    print(f\"No tweets found for keyword: {keyword}\")\n",
        "    exit()\n",
        "\n",
        "# Tokenization, Lemmatization, and cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "all_words = []\n",
        "for text in data['text']:\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha() and word not in stop_words]\n",
        "    all_words.extend(words)\n",
        "\n",
        "# Incorporate bigrams\n",
        "all_bigrams = list(bigrams(all_words))\n",
        "bigram_freq = Counter(all_bigrams)\n",
        "top_bigrams = bigram_freq.most_common(top_n)\n",
        "\n",
        "# Get the most common words associated with the keyword\n",
        "word_freq = Counter(all_words)\n",
        "trends = word_freq.most_common(top_n)  # Top N trends\n",
        "\n",
        "# If no tweets found for the given keyword, exit\n",
        "if data.empty:\n",
        "    print(f\"No tweets found for keyword: {keyword}\")\n",
        "    exit()\n",
        "\n",
        "# ... your tokenization and cleaning code ...\n",
        "\n",
        "# Check if trends list is empty\n",
        "if not trends:\n",
        "    print(f\"No significant words found for the keyword: {keyword} in the sampled dataset.\")\n",
        "else:\n",
        "    # Visualization\n",
        "    words, counts = zip(*trends)\n",
        "    plt.figure(figsize=(15,7))\n",
        "    plt.bar(words, counts, color='skyblue')\n",
        "    plt.title(f\"Top {top_n} Trends associated with '{keyword}'\")\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Display top bigrams\n",
        "print(f\"\\nTop {top_n} Bigrams associated with '{keyword}':\")\n",
        "for bigram, freq in top_bigrams:\n",
        "    print(f\"{bigram}: {freq}\")\n",
        "\n",
        "# Sentiment Analysis\n",
        "positive_tweets = len(data[data['sentiment'] == 4])\n",
        "neutral_tweets = len(data[data['sentiment'] == 2])\n",
        "negative_tweets = len(data[data['sentiment'] == 0])\n",
        "\n",
        "print(f\"\\nSentiment Analysis for '{keyword}':\")\n",
        "print(f\"Positive Tweets: {positive_tweets}\")\n",
        "print(f\"Neutral Tweets: {neutral_tweets}\")\n",
        "print(f\"Negative Tweets: {negative_tweets}\")\n"
      ],
      "metadata": {
        "id": "qMY035255jsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code performs keyword trend analysis and sentiment analysis on a dataset of tweets stored in Google Drive. The main objective of this code is to analyse how frequently certain words and bigrams appear in tweets containing a specific keyword entered by the user and to determine the sentiment of those tweets.\n",
        "\n",
        "Libraries user:\n",
        "**pandas** for data manipulation and analysis.\n",
        "**Counter** to count occurrences of elements.\n",
        "Several functions and modules from the **nltk** library for natural language processing.\n",
        "**matplotlib.pyplot** for data visualization.\n",
        "\n",
        "**NLTK Data Download:**\n",
        "It downloads the necessary data files for the **nltk** library. This includes stopwords, tokenizers, lemmatizers, and POS taggers.\n",
        "\n",
        "**User Input**:\n",
        "The code takes two inputs from the user:\n",
        "A keyword to analyze trends.\n",
        "The number of top trends (words or bigrams) they want to view.\n",
        "\n",
        "**Tokenization, Lemmatization, and Cleaning:**\n",
        "The code performs several text preprocessing steps:\n",
        "Tokenization: splitting text into individual words.\n",
        "Lemmatization: converting words to their base form (e.g., \"running\" to \"run\").\n",
        "Cleaning: removing non-alphanumeric words and stopwords (common words like \"and\", \"the\", etc.).\n",
        "**Bigram Analysis:**\n",
        "After tokenization, the code creates bigrams (pairs of adjacent words) and counts their occurrences.\n",
        "\n",
        "**Top Trends:**\n",
        "The code identifies the top N words (trends) associated with the input keyword.\n",
        "** Visualization:**\n",
        "The top N trends are visualized using a bar chart.\n",
        "\n",
        "**Sentiment Analysis:**\n",
        "The code counts the number of positive, neutral, and negative tweets associated with the keyword and prints out the counts for each sentiment category.\n"
      ],
      "metadata": {
        "id": "imtIOwCC9GzJ"
      }
    }
  ]
}